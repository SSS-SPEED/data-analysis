
在Python2版本中，有urllib和urlib2两个库可以用来实现request的发送。
在Python3中，已经不存在urllib2这个库了，统一为urllib。


import urllib

分成4块：
urllib.request          发送request和获取request的结果
urllib.error            包含了urllib.request产生的异常
urllib.parse            用来解析和处理URL
urllib.robotparser      解析页面的robots.txt文件



urllib.request：

☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ request.urlopen ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ 简单方式

result = urllib.request.urlopen("https://www.baidu.com")       # 基本使用
print(result.read().decode("utf-8"))                           # 打印出这个网站的内容，加utf-8可读性增强
print(result.status)                                           # 获取响应的状态码
print(result.getheaders())                                     # 获取响应的头信息

完整：
urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)

data：附加参数，必须bytes类型，而且一旦加了就是POST请求
      data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')     # 键值对，utf-8编码，转化成bytes类型
      response = urllib.request.urlopen('http://httpbin.org/post', data=data)      # post请求

[timeout,]*：设置超时时间，单位是秒
             response = urllib.request.urlopen("http://httpbin.org/get",timeout=1)     # 注意形式是timeout=1
             print(response.read().decode("utf-8")) 
关于超时的错误处理：
try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):                                           # 注意导入import socket
        print(e.reason)                                                                # 是timed out

context：指定SSL设置，必须是ssl.SSLContext类型
cafile和capath：指定CA证书和它的路径，HTTPS 链接时会有用
cadefault：已经弃用，默认False


☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ request.urlopen ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ 复杂方式

request = urllib.request.Request("https://www.baidu.com")        # 仅仅是urlopen的变种
response = urllib.request.urlopen(request)

完整：
urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) 
可以通过help (urllib.request.Request)查看

data：附加参数，必须bytes类型，而且一旦加了就是POST请求

headers：请求的头部，也可以通过add_header()的方式添加
         例如，修改User-Agent来伪装浏览器
         Agent是Python-urllib，如果改成Mozilla/5.0 (X11; U; Linux i686)Gecko/20071127 Firefox/2.0.0.11，就是火狐了

origin_req_host：指的是请求方的 host 名称或者 IP 地址

unverifiable：指的是这个请求是否是无法验证的，默认是False。
              意思就是说用户没有足够权限来选择接收这个请求的结果。
              例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable的值就是True

method：字符串，指示请求使用的方法，比如GET，POST，PUT等等

示例！！！！！
url = "http://httpbin.org/post"                                      # 目标url
headers = {                                                          # 设置请求的头部
    "User-Agent":'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',   # 伪装一个火狐浏览器  
    "host":'httpbin.org'  
}  
dict = {  
    "name":"Germey"                                                  # post请求的内容
}  
data = bytes(urllib.parse.urlencode(dict),encoding="utf8")           # 转化成字节流
req = urllib.request.Request(url=url,data=data,headers=headers,method="POST")    # 组成urllib.request.Request
# req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5;Windows NT)')  # 继续添加头部的方法
response = urllib.request.urlopen(req)                                           # urlopen这个Request
print(response.read().decode("utf-8"))



☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ Handler ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ 高级用法 （urllib.request里面的各种Handler）

HTTPDefaultErrorHandler：        用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常。
HTTPRedirectHandler：            用于处理重定向。
HTTPCookieProcessor：            用于处理Cookies。
ProxyHandler：                   用于设置代理，默认代理为空。
HTTPPasswordMgr：                用于管理密码，它维护了用户名和密码的表。
HTTPBasicAuthHandler：           用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。

最后用Handler来构建Opener，Opener就是高级的urllib.request.Request


/////////////////////////// 示例1 需要身份验证的网页 ///////////////////////////

from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError                   # from...import... 可以直接用URLError代替urllib.error.URLError

username = 'username'                               # 参数
password = 'password'
url = 'http://localhost:5000/'

p=HTTPPasswordMgrWithDefaultRealm()                 # HTTPBasicAuthHandler需要这个参数，所以实例化
p.add_password(None,url,username,password)          # 为这个参数添加必要的参数
auth_handler=HTTPBasicAuthHandler(p)                # HTTPBasicAuthHandler的实例化
opener=build_opener(auth_handler)                   # 利用这个Handler并使用build_opener()方法构建一个Opener

try:
    result=opener.open(url)                         # Opener通过open方法打开链接，完成验证
    html=result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)


/////////////////////////// 示例2 代理 ///////////////////////////

from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener
 
proxy_handler = ProxyHandler({                         # 使用ProxyHandler，其参数是一个字典，
                                                       # 键名是协议类型（比如HTTP或者HTTPS等），键值是代理链接，可以添加多个代理。
    'http': 'http://127.0.0.1:9743',                   # 这里我们在本地搭建了一个代理，它运行在9743端口上
    'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)                   # 利用这个Handler及build_opener()方法构造一个Opener，之后发送请求即可
try:
    response = opener.open('https://www.baidu.com')
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)


/////////////////////////// 示例3 Cookies ///////////////////////////

import http.cookiejar, urllib.request
 
cookie = http.cookiejar.CookieJar()                       # 首先，我们必须声明一个CookieJar对象
handler = urllib.request.HTTPCookieProcessor(cookie)      # 用这个对象来构建一个HTTPCookieProcessor
opener = urllib.request.build_opener(handler)             # 然后利用build_opener()方法构建出Opener
response = opener.open('http://www.baidu.com')            # 最后执行open()函数即可
for item in cookie:
    print(item.name+"="+item.value)



☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ 错误的处理 ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆

from urllib import request, error
import socket
 
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:                                            # 优先处理HTTPError错误，能得到详细错误信息
    print(e.reason, e.code, e.headers, sep='\n')                        # sep='\n'是分隔符，将前面的参数用换行来分割开来
except error.URLError as e:                                             # 之后才是URLError错误
    if isinstance(e.reason, socket.timeout):                            # 一些错误，例如超时，HTTPError捕捉不到，则要单独写
        print('TIME OUT')
    else:
         print(e.reason)                                                # 只有简单的错误提示
else:
    print('Request Successfully')                                       # 一点错都没，则进入这里
 


☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ urllib.parse ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆

urllib.parse定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换

/////////////////////////// urllib.parse.urlparse ///////////////////////////

from urllib.parse import urlparse
result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print( result)

结果是：返回6个
ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')

scheme://netloc/path;parameters?query#fragment
一个标准的URL都会符合这个规则，利用urlparse()方法可以将它拆分开来。

协议   scheme='http'
域名   netloc='www.baidu.com'
路径   path='/index.html'


/////////////////////////// urllib.parse.urlunparse ///////////////////////////

from urllib.parse import urlunparse             
 
data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))

输出：http://www.baidu.com/index.html;user?a=6#comment         # 其实就是自己组成一个URL


/////////////////////////// urllib.parse.urlparse 与 urllib.parse.urlunsplit ///////////////////////////

result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result[0])            # 与urlparse类似，但是返回只有5个，将path与params合并到了path

data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))     # 与urlunparse类似


/////////////////////////// urllib.parse.urljoin ///////////////////////////

通过urljoin()方法，我们可以轻松实现链接的解析、拼合与生成。

print(urljoin('http://www.baidu.com', 'FAQ.html'))       # 输出http://www.baidu.com/FAQ.html
print(urljoin('www.baidu.com#comment', '?category=2'))   # 输出www.baidu.com?category=2
第一个参数是base_url，仅提供scheme、netloc和path
第二个参数是需要拼接的url，如果缺失scheme、netloc或path，刚好base_url有提供，就补上


/////////////////////////// ☆☆☆ urllib.parse.urlencode ☆☆☆ ///////////////////////////

将字典参数变成GET请求参数，非常方便

from urllib.parse import urlencode
 
params = {
    'name': 'germey',                   # 原本是字典，拼成url很麻烦
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)      # 使用urlencode拼接就行了
print(url)


/////////////////////////// parse_qs与parse_qsl ///////////////////////////

from urllib.parse import parse_qs
 
query = 'name=germey&age=22'
print(parse_qs(query))                  # {'name': ['germey'], 'age': ['22']}，转回了字典，与urlencode相反
print(parse_qsl(query))                 # [('name', 'germey'), ('age', '22')]，转回了元祖


/////////////////////////// quote与unquote ///////////////////////////

URL的编码与解码

from urllib.parse import quote, unquote
 
keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)     # 将这个中文参数进行URL编码
print(url)                                               # 输出https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
print(unquote(url))                                      # 又解码回来了~



☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆ Robots协议 ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆

Robots协议：通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下
            作用是告诉搜索引擎（也告诉了爬虫）哪些页面可以抓取，哪些不可以抓取

robots.txt的样例：
User-agent: *         # 描述了搜索爬虫的名称，*代表该协议对任何爬取爬虫有效
Disallow: /           # /代表不允许抓取所有页面。
Allow: /public/       # 只允许爬取public目录

一些常见搜索爬虫的名称及其对应的网站：
BaiduSpider         百度            www.baidu.com
Googlebot           谷歌            www.google.com
360Spider           360搜索         www.so.com
YodaoBot            有道            www.youdao.com
ia_archiver         Alexa           www.alexa.cn
Scooter             altavista       www.altavista.com

解析robots.txt，以简书为例
from urllib.robotparser import RobotFileParser
 
rp = RobotFileParser()                                  # 创建实例，也可以直接rp = RobotFileParser('http://www.jianshu.com/robots.txt')
rp.set_url('http://www.jianshu.com/robots.txt')         # 为RobotFileParser设置robots.txt的链接
rp.read()                                               # 执行读取和分析操作，一定要调用，否则调用其他方法返回都是false
# 也可以用parse()方法执行读取和分析
# rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))                               # 返回True，表示可以抓取
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))      # 返回False，表示不可以抓取

can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。
             返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False





